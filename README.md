
# ğŸ”¥ Sequence-to-Sequence Event Prediction

ğŸš€ **An advanced deep learning-based sequence-to-sequence (Seq2Seq) model for event prediction with cross-validation, hyperparameter tuning, and robust evaluation.**

ğŸ”— **GitHub Repository:** [Sequence-to-Sequence Event Prediction](https://github.com/ShalinVachheta017/sequence-2-sequence-Event-Prediction/tree/main)

---

## ğŸ“Œ **Project Overview**
This repository implements a **sequence-to-sequence (Seq2Seq)** model using **LSTM** networks to predict event sequences based on historical data. It supports:
- **K-Fold Cross-Validation** for model robustness.
- **Hyperparameter tuning** to find the best configuration.
- **Post-training analysis** to select the best model.
- **Retraining & Evaluation** on unseen test data.

---

## ğŸ— **Project Structure**
```
ğŸ“‚ Sequence-to-Sequence-Event-Prediction
â”‚â”€â”€ main.py              # Runs the full training pipeline
â”‚â”€â”€ train.py             # Training & validation functions
â”‚â”€â”€ post_analysis.py     # Selects the best model after training
â”‚â”€â”€ retrain.py           # Retrains the best model & evaluates on test set
â”‚â”€â”€ model_definition.py  # LSTM-based Encoder-Decoder model
â”‚â”€â”€ data_loader.py       # Data preprocessing & k-fold dataset preparation
â”‚â”€â”€ metrics.py           # Custom loss & evaluation metrics
â”‚â”€â”€ utils.py             # Utility functions (logging, config handling, plotting)
â”‚â”€â”€ config.yaml          # Configuration settings for training jobs
â”‚â”€â”€ Events.csv           # Dataset file (input)
â”‚â”€â”€ README.md            # Project Documentation
```

---

## âš™ï¸ **Installation & Setup**
### **1ï¸âƒ£ Clone the Repository**
```bash
git clone https://github.com/ShalinVachheta017/sequence-2-sequence-Event-Prediction.git
cd sequence-2-sequence-Event-Prediction
```

### **2ï¸âƒ£ Install Dependencies**
```bash
pip install -r requirements.txt
```

### **3ï¸âƒ£ Ensure Correct Folder Structure**
Ensure that `Events.csv` (dataset file) is present in the root directory.

---

## ğŸš€ **How to Train the Model**
The training pipeline performs:
- **Data Loading & Preprocessing**
- **K-Fold Cross-Validation**
- **Hyperparameter Optimization**
- **Performance Metrics Computation**

#### **Run the Training Script**
```bash
python main.py
```
This script:
1. Loads data from `Events.csv`
2. Trains the model with multiple hyperparameters (specified in `config.yaml`).
3. Saves training logs and model outputs in `./logs` and `./outputs` directories.

---

## ğŸ“Š **Post-Training Analysis**
To analyze the trained models and select the best-performing one:
```bash
python post_analysis.py
```
This script selects the best model based on **validation coverage and accuracy** while avoiding overfitting.

---

## ğŸ”„ **Retraining and Final Evaluation**
Once the best model is selected, it can be retrained multiple times and evaluated on test data.

```bash
python retrain.py
```
This script:
1. Loads the **best model configuration**.
2. Retrains it multiple times for robustness.
3. Evaluates on the test set.
4. Saves test performance results in `final_test_results.csv`.

---

## ğŸ›  **Customization**
To modify the training configurations (batch size, learning rate, epochs, etc.), edit **`config.yaml`**:
```yaml
general:
  batch_size: 8
  epochs_list: [40, 80, 100, 120]
  learning_rates: [0.001]
  hidden_sizes: [64, 128, 256]
  num_layers_options: [1, 2, 3]
  bidirectional_options: [true, false]
```

---

## ğŸ“ˆ **Performance Metrics**
The model is evaluated using:
- **Cross-Entropy Loss** for training and validation.
- **Accuracy & Coverage** for post-training and evaluation.

The results are stored in:
- `all_jobs_results.csv` (training phase)
- `final_test_results.csv` (final evaluation)

---

## ğŸ¤– **Model Architecture**
The model is a **Bidirectional LSTM-based Encoder-Decoder**:
```
Encoder (LSTM) â†’ Decoder (LSTM) â†’ Fully Connected Layer
```
The decoder uses **teacher forcing** with a ratio of `0.3` during training.

---

## ğŸ† **Results & Visualization**
- Training loss curves, validation loss curves, and accuracy  are saved in `./outputs/`

---

## ğŸ“¬ **Contributing**
Feel free to **fork**, **open issues**, or submit **pull requests**! ğŸ˜Š

---

## ğŸ“ **License**
This project is open-source under the **MIT License**.

